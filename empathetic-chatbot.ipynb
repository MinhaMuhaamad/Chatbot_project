{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3238154,"sourceType":"datasetVersion","datasetId":1962861}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================\n# STEP 0: SETUP & GPU CHECK\n# ============================================\n\nimport os\nimport re\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n\n# Check GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"‚úÖ Using device: {device}\")\n\n# Fix seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T05:39:46.304397Z","iopub.execute_input":"2025-10-15T05:39:46.305227Z","iopub.status.idle":"2025-10-15T05:39:46.312173Z","shell.execute_reply.started":"2025-10-15T05:39:46.305191Z","shell.execute_reply":"2025-10-15T05:39:46.311436Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Using device: cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================\n# STEP 0‚Äì2: LOAD & CLEAN DATASET (CUSTOMIZED FOR EMPATHETIC DIALOGUES)\n# ============================================\n\nimport os\nimport pandas as pd\nimport re\nimport glob\n\nprint(\"üîç DEBUGGING: Searching for dataset files...\\n\")\n\n# --------------------------------------------\n# Locate all CSVs under /kaggle/input\n# --------------------------------------------\ninput_path = \"/kaggle/input\"\ncsv_files = glob.glob(f\"{input_path}/**/*.csv\", recursive=True)\n\nif not csv_files:\n    raise FileNotFoundError(\"‚ùå No CSV files found in /kaggle/input! Please add dataset.\")\nelse:\n    print(f\"‚úÖ Found {len(csv_files)} CSV file(s):\")\n    for csv_file in csv_files:\n        size_mb = os.path.getsize(csv_file) / (1024*1024)\n        print(f\"  üìä {csv_file} ({size_mb:.2f} MB)\")\n    print()\n\n# --------------------------------------------\n# Load dataset(s)\n# --------------------------------------------\ndfs = []\nfor csv_file in csv_files:\n    try:\n        temp_df = pd.read_csv(csv_file)\n        print(f\"‚úÖ Loaded: {os.path.basename(csv_file)} ‚Üí shape {temp_df.shape}\")\n        dfs.append(temp_df)\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Skipping {csv_file} due to error: {e}\")\n\nif not dfs:\n    raise ValueError(\"‚ùå No valid CSV files could be loaded.\")\n\n# Merge datasets if multiple files exist\ndf = pd.concat(dfs, ignore_index=True)\nprint(f\"\\nüì¶ Final combined dataset shape: {df.shape}\")\n\nprint(\"\\nüîé Columns detected:\")\nprint(list(df.columns))\n\n# --------------------------------------------\n# Manual column mapping (your dataset structure)\n# --------------------------------------------\ncontext_col = \"Situation\"\nemotion_col = \"emotion\"\ncustomer_col = \"empathetic_dialogues\"\nagent_col = \"labels\"\n\n# Safety check\nfor col in [context_col, emotion_col, customer_col, agent_col]:\n    if col not in df.columns:\n        raise ValueError(f\"‚ùå Column '{col}' not found in dataset! Check column names.\")\n\nprint(\"\\nüß≠ Column mapping:\")\nprint(f\"  - Situation/Context: {context_col}\")\nprint(f\"  - Customer: {customer_col}\")\nprint(f\"  - Agent: {agent_col}\")\nprint(f\"  - Emotion: {emotion_col}\")\n\n# --------------------------------------------\n# Clean text safely (preserve important chars)\n# --------------------------------------------\ndef normalize_text(text):\n    text = str(text).lower().strip()\n    text = re.sub(r\"\\s+\", \" \", text)               # normalize spaces\n    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)      # space punctuation\n    text = re.sub(r\"[^a-z0-9?.!,‚Äô' ]+\", \" \", text) # keep numbers & apostrophes\n    return text.strip()\n\nfor col in [context_col, customer_col, agent_col, emotion_col]:\n    df[col] = df[col].astype(str).apply(normalize_text)\n\nprint(\"\\n‚úÖ Text normalization completed.\")\nprint(f\"üßæ Sample rows:\\n{df[[emotion_col, context_col, customer_col, agent_col]].sample(2, random_state=42)}\")\n\n# --------------------------------------------\n# Basic sanity checks\n# --------------------------------------------\nmissing = df[[context_col, customer_col, agent_col, emotion_col]].isnull().sum().sum()\nif missing > 0:\n    print(f\"‚ö†Ô∏è Warning: Found {missing} missing text values. Filling with empty strings.\")\n    df.fillna(\"\", inplace=True)\n\nprint(\"\\n‚úÖ Dataset is ready for tokenization.\")\nprint(f\"Total records: {len(df)}\")\n\n# Save variable names for later steps\nCOLUMNS = {\n    \"emotion\": emotion_col,\n    \"context\": context_col,\n    \"customer\": customer_col,\n    \"agent\": agent_col\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T05:39:52.639918Z","iopub.execute_input":"2025-10-15T05:39:52.640463Z","iopub.status.idle":"2025-10-15T05:39:55.627208Z","shell.execute_reply.started":"2025-10-15T05:39:52.640437Z","shell.execute_reply":"2025-10-15T05:39:55.626502Z"}},"outputs":[{"name":"stdout","text":"üîç DEBUGGING: Searching for dataset files...\n\n‚úÖ Found 1 CSV file(s):\n  üìä /kaggle/input/empathetic-dialogues-facebook-ai/emotion-emotion_69k.csv (15.95 MB)\n\n‚úÖ Loaded: emotion-emotion_69k.csv ‚Üí shape (64636, 7)\n\nüì¶ Final combined dataset shape: (64636, 7)\n\nüîé Columns detected:\n['Unnamed: 0', 'Situation', 'emotion', 'empathetic_dialogues', 'labels', 'Unnamed: 5', 'Unnamed: 6']\n\nüß≠ Column mapping:\n  - Situation/Context: Situation\n  - Customer: empathetic_dialogues\n  - Agent: labels\n  - Emotion: emotion\n\n‚úÖ Text normalization completed.\nüßæ Sample rows:\n            emotion                                          Situation  \\\n37054       content  my husband and i have been married 22 years . ...   \n15618  anticipating                 i can't wait to get to the ocean !   \n\n                                    empathetic_dialogues  \\\n37054  customer  no ,  just married kinda young ,  bu...   \n15618  customer  no ,  i've never done that but my fa...   \n\n                                                  labels  \n37054  that is a great accomplishment since most marr...  \n15618                 ah ,  gotcha .  sounds delicious !  \n\n‚úÖ Dataset is ready for tokenization.\nTotal records: 64636\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# STEP 3: TOKENIZATION + VOCABULARY","metadata":{}},{"cell_type":"code","source":"# ============================================\n# STEP 3: TOKENIZATION & VOCABULARY CREATION\n# ============================================\n\nfrom collections import Counter\nfrom tqdm.notebook import tqdm\n\nprint(\"üî° Building vocabulary from dataset...\\n\")\n\n# Use columns from previous step\nemotion_col = COLUMNS[\"emotion\"]\ncontext_col = COLUMNS[\"context\"]\ncustomer_col = COLUMNS[\"customer\"]\nagent_col = COLUMNS[\"agent\"]\n\nSPECIAL_TOKENS = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\n\ndef tokenize(sentence):\n    \"\"\"Simple space-based tokenizer.\"\"\"\n    return sentence.split()\n\nword_counts = Counter()\n\n# Combine all dataset text and count words\nfor _, row in tqdm(df.iterrows(), total=len(df)):\n    emotion = row.get(emotion_col, \"\")\n    context = row.get(context_col, \"\")\n    customer = row.get(customer_col, \"\")\n    agent = row.get(agent_col, \"\")\n    \n    # Merge all relevant parts for tokenization\n    text = f\"emotion: {emotion} | situation: {context} | customer: {customer} agent: {agent}\"\n    word_counts.update(tokenize(text))\n\n# Minimum frequency filter (to avoid rare typos/noise)\nmin_freq = 2\nvocab = SPECIAL_TOKENS + [w for w, c in word_counts.items() if c >= min_freq]\n\nword2idx = {w: i for i, w in enumerate(vocab)}\nidx2word = {i: w for w, i in word2idx.items()}\n\nprint(f\"‚úÖ Vocabulary built successfully!\")\nprint(f\"üìä Total unique words (min_freq ‚â• {min_freq}): {len(vocab)}\")\n\n# Show sample tokens\nprint(\"\\nüî§ Sample tokens:\", vocab[:25])\n\n# Basic stats\nrare_words = sum(1 for w, c in word_counts.items() if c < min_freq)\nprint(f\"‚öôÔ∏è Rare words filtered out (<{min_freq}): {rare_words}\")\nprint(f\"üìö Final vocabulary size: {len(vocab)}\")\n\n# Save for later use\nVOCAB = {\n    \"vocab\": vocab,\n    \"word2idx\": word2idx,\n    \"idx2word\": idx2word\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T05:40:03.418353Z","iopub.execute_input":"2025-10-15T05:40:03.418691Z","iopub.status.idle":"2025-10-15T05:40:07.055732Z","shell.execute_reply.started":"2025-10-15T05:40:03.418668Z","shell.execute_reply":"2025-10-15T05:40:07.054857Z"}},"outputs":[{"name":"stdout","text":"üî° Building vocabulary from dataset...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/64636 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48c1ed5a6d1f4430b542b395a56afdf2"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Vocabulary built successfully!\nüìä Total unique words (min_freq ‚â• 2): 17800\n\nüî§ Sample tokens: ['<pad>', '<bos>', '<eos>', '<unk>', 'emotion:', 'sentimental', '|', 'situation:', 'i', 'remember', 'going', 'to', 'the', 'fireworks', 'with', 'my', 'best', 'friend', '.', 'there', 'was', 'a', 'lot', 'of', 'people']\n‚öôÔ∏è Rare words filtered out (<2): 2655\nüìö Final vocabulary size: 17800\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ============================================================\n# SAVE VOCABULARY TO FILE (for Streamlit app)\n# ============================================================\nimport pickle\n\nVOCAB = {\n    \"word2idx\": word2idx,\n    \"idx2word\": idx2word\n}\n\nwith open(\"/kaggle/working/vocab.pkl\", \"wb\") as f:\n    pickle.dump(VOCAB, f)\n\nprint(\"‚úÖ Vocabulary saved successfully at /kaggle/working/vocab.pkl\")\nprint(f\"üî§ Total vocab size: {len(word2idx)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T05:40:14.104268Z","iopub.execute_input":"2025-10-15T05:40:14.104551Z","iopub.status.idle":"2025-10-15T05:40:14.115068Z","shell.execute_reply.started":"2025-10-15T05:40:14.104529Z","shell.execute_reply":"2025-10-15T05:40:14.114159Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Vocabulary saved successfully at /kaggle/working/vocab.pkl\nüî§ Total vocab size: 17800\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# STEP 4: SPLITTING + ENCODING + PADDING","metadata":{}},{"cell_type":"code","source":"# ============================================\n# STEP 4: DATA SPLIT + ENCODING + PADDING\n# ============================================\n\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\n\nprint(\"üîÄ Splitting dataset and encoding sequences...\\n\")\n\n# ----------------------------------------------------\n# Prepare Input (X) and Target (Y)\n# ----------------------------------------------------\nemotion_col = COLUMNS[\"emotion\"]\ncontext_col = COLUMNS[\"context\"]\ncustomer_col = COLUMNS[\"customer\"]\nagent_col = COLUMNS[\"agent\"]\n\ninputs, targets = [], []\n\nfor _, row in tqdm(df.iterrows(), total=len(df)):\n    emotion = row[emotion_col]\n    context = row[context_col]\n    customer = row[customer_col]\n    agent = row[agent_col]\n    \n    x = f\"emotion: {emotion} | situation: {context} | customer: {customer} agent:\"\n    y = agent\n    \n    inputs.append(x)\n    targets.append(y)\n\n# ----------------------------------------------------\n# Train/Val/Test Split (80/10/10)\n# ----------------------------------------------------\nX_train, X_temp, y_train, y_temp = train_test_split(inputs, targets, test_size=0.2, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\nprint(f\"‚úÖ Split complete:\")\nprint(f\"  ‚Ä¢ Train: {len(X_train)}\")\nprint(f\"  ‚Ä¢ Val:   {len(X_val)}\")\nprint(f\"  ‚Ä¢ Test:  {len(X_test)}\")\n\n# ----------------------------------------------------\n# Convert text ‚Üí token IDs\n# ----------------------------------------------------\ndef encode_sentence(sentence, word2idx, max_len=64):\n    tokens = sentence.split()\n    ids = [word2idx.get(tok, word2idx[\"<unk>\"]) for tok in tokens]\n    ids = [word2idx[\"<bos>\"]] + ids + [word2idx[\"<eos>\"]]  # add special tokens\n    if len(ids) < max_len:\n        ids += [word2idx[\"<pad>\"]] * (max_len - len(ids))\n    else:\n        ids = ids[:max_len]\n    return ids\n\nmax_len_input = 64\nmax_len_output = 64\n\nprint(\"\\nüî¢ Encoding sequences...\")\n\ndef encode_dataset(X, Y):\n    X_encoded = [encode_sentence(x, VOCAB[\"word2idx\"], max_len_input) for x in X]\n    Y_encoded = [encode_sentence(y, VOCAB[\"word2idx\"], max_len_output) for y in Y]\n    return torch.tensor(X_encoded), torch.tensor(Y_encoded)\n\nX_train_enc, y_train_enc = encode_dataset(X_train, y_train)\nX_val_enc, y_val_enc = encode_dataset(X_val, y_val)\nX_test_enc, y_test_enc = encode_dataset(X_test, y_test)\n\nprint(\"\\n‚úÖ Encoding complete!\")\nprint(f\"Train set tensor shapes: X={X_train_enc.shape}, Y={y_train_enc.shape}\")\n\n# ----------------------------------------------------\n# Sample decoded check\n# ----------------------------------------------------\ndef decode(ids):\n    words = [VOCAB[\"idx2word\"][i] for i in ids if i != VOCAB[\"word2idx\"][\"<pad>\"]]\n    return \" \".join(words)\n\nprint(\"\\nüßæ Example check:\")\nprint(\"Input :\", decode(X_train_enc[0].tolist()))\nprint(\"Target:\", decode(y_train_enc[0].tolist()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:32:14.238027Z","iopub.execute_input":"2025-10-14T10:32:14.238834Z","iopub.status.idle":"2025-10-14T10:32:19.445210Z","shell.execute_reply.started":"2025-10-14T10:32:14.238808Z","shell.execute_reply":"2025-10-14T10:32:19.444420Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# STEP 5: TRANSFORMER MODEL (Encoder‚ÄìDecoder) From Scratch","metadata":{}},{"cell_type":"code","source":"# ============================================\n# STEP 5: TRANSFORMER ENCODER‚ÄìDECODER FROM SCRATCH\n# ============================================\n\nimport torch\nimport torch.nn as nn\nimport math\n\n# --------------------------------------------------------\n# ‚úÖ GPU SETUP\n# --------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"‚öôÔ∏è Using device: {device}\")\n\n# --------------------------------------------------------\n# üîπ Positional Encoding\n# --------------------------------------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1), :]\n        return x\n\n# --------------------------------------------------------\n# üîπ Multi-Head Attention\n# --------------------------------------------------------\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n\n        self.linear_q = nn.Linear(d_model, d_model)\n        self.linear_k = nn.Linear(d_model, d_model)\n        self.linear_v = nn.Linear(d_model, d_model)\n        self.linear_out = nn.Linear(d_model, d_model)\n\n        self.scale = math.sqrt(self.d_k)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n\n        Q = self.linear_q(query)\n        K = self.linear_k(key)\n        V = self.linear_v(value)\n\n        # (batch, heads, seq_len, d_k)\n        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        attn = torch.softmax(scores, dim=-1)\n        context = torch.matmul(attn, V)\n\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        output = self.linear_out(context)\n        return output, attn\n\n# --------------------------------------------------------\n# üîπ Feed-Forward Network\n# --------------------------------------------------------\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=512, dropout=0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.fc2(self.dropout(torch.relu(self.fc1(x))))\n\n# --------------------------------------------------------\n# üîπ Encoder Layer\n# --------------------------------------------------------\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff=512, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.ff = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        attn_out, _ = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_out))\n        ff_out = self.ff(x)\n        x = self.norm2(x + self.dropout(ff_out))\n        return x\n\n# --------------------------------------------------------\n# üîπ Decoder Layer\n# --------------------------------------------------------\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff=512, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.ff = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n        _x, _ = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(_x))\n        _x, attn_weights = self.cross_attn(x, enc_out, enc_out, src_mask)\n        x = self.norm2(x + self.dropout(_x))\n        ff_out = self.ff(x)\n        x = self.norm3(x + self.dropout(ff_out))\n        return x, attn_weights\n\n# --------------------------------------------------------\n# üîπ Full Transformer\n# --------------------------------------------------------\nclass TransformerChatbot(nn.Module):\n    def __init__(self, vocab_size, d_model=256, num_heads=2, num_layers=2, dropout=0.1, pad_idx=0):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n        self.pos_encoder = PositionalEncoding(d_model)\n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_model*2, dropout) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_model*2, dropout) for _ in range(num_layers)])\n        self.fc_out = nn.Linear(d_model, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        self.d_model = d_model\n        self.pad_idx = pad_idx\n\n    def make_subsequent_mask(self, size):\n        mask = torch.tril(torch.ones(size, size)).bool().to(device)\n        return mask\n\n    def forward(self, src, tgt):\n        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n        tgt_mask = self.make_subsequent_mask(tgt.size(1))\n\n        src_embed = self.dropout(self.pos_encoder(self.embedding(src)))\n        tgt_embed = self.dropout(self.pos_encoder(self.embedding(tgt)))\n\n        enc_out = src_embed\n        for layer in self.encoder_layers:\n            enc_out = layer(enc_out, src_mask)\n\n        dec_out = tgt_embed\n        for layer in self.decoder_layers:\n            dec_out, _ = layer(dec_out, enc_out, src_mask, tgt_mask)\n\n        logits = self.fc_out(dec_out)\n        return logits\n\n# --------------------------------------------------------\n# üîπ Model Initialization\n# --------------------------------------------------------\n# --------------------------------------------------------\n# ‚úÖ Ensure vocabulary variables are defined\n# --------------------------------------------------------\n\n# If you already have vocab built in memory, skip this block.\n# Otherwise, reconstruct from your tokenizer step.\n\ntry:\n    vocab\n    word2idx\n    idx2word\n    print(\"‚úÖ Vocabulary variables already exist in memory.\")\nexcept NameError:\n    print(\"‚ö†Ô∏è vocab not found ‚Äî reconstructing minimal example...\")\n\n    # Example: reload from saved files (if you saved them earlier)\n    # vocab = torch.load(\"/kaggle/working/vocab.pt\")\n    # word2idx = torch.load(\"/kaggle/working/word2idx.pt\")\n    # idx2word = torch.load(\"/kaggle/working/idx2word.pt\")\n\n    # OR temporary fallback (if not saved)\n    vocab = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\n    word2idx = {w: i for i, w in enumerate(vocab)}\n    idx2word = {i: w for w, i in word2idx.items()}\nVOCAB = {\n    \"vocab\": vocab,\n    \"word2idx\": word2idx,\n    \"idx2word\": idx2word\n}\nvocab_size = len(VOCAB[\"word2idx\"])\npad_idx = VOCAB[\"word2idx\"][\"<pad>\"]\n\nmodel = TransformerChatbot(\n    vocab_size=vocab_size,\n    d_model=256,\n    num_heads=2,\n    num_layers=2,\n    dropout=0.1,\n    pad_idx=pad_idx\n).to(device)\n\nprint(\"\\n‚úÖ Model initialized successfully!\")\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:32:19.446704Z","iopub.execute_input":"2025-10-14T10:32:19.446899Z","iopub.status.idle":"2025-10-14T10:32:19.589242Z","shell.execute_reply.started":"2025-10-14T10:32:19.446885Z","shell.execute_reply":"2025-10-14T10:32:19.588503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"Device count:\", torch.cuda.device_count())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:32:19.590090Z","iopub.execute_input":"2025-10-14T10:32:19.590368Z","iopub.status.idle":"2025-10-14T10:32:19.594929Z","shell.execute_reply.started":"2025-10-14T10:32:19.590345Z","shell.execute_reply":"2025-10-14T10:32:19.594248Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# testing training code for small dataset","metadata":{}},{"cell_type":"code","source":"# # ============================================\n# # STEP 6 (FIXED): MINI TRAINING TEST (10% DATA)\n# # ============================================\n\n# import torch\n# from torch.utils.data import Dataset, DataLoader\n# import torch.nn as nn\n# import torch.optim as optim\n# import pandas as pd\n# from tqdm.notebook import tqdm\n# import gc\n\n# # --------------------------------------------------------\n# # ‚öôÔ∏è CONFIGURATION\n# # --------------------------------------------------------\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"üöÄ Using device: {device}\")\n\n# BATCH_SIZE = 16\n# EPOCHS = 1\n# LR = 3e-4\n# MAX_LEN = 64\n\n# # --------------------------------------------------------\n# # üßæ LOAD MAIN DATAFRAME (already used earlier)\n# # --------------------------------------------------------\n# # if df not loaded in memory, reload from your dataset file:\n# try:\n#     df\n# except NameError:\n#     df = pd.read_csv(\"/kaggle/input/empathetic-dialogues-facebook-ai/emotion-emotion_69k.csv\")\n\n# print(f\"üìä Loaded dataset shape: {df.shape}\")\n\n# # --------------------------------------------------------\n# # üîß Define column mapping (based on your earlier debug)\n# # --------------------------------------------------------\n# COLUMNS = {\n#     \"emotion\": \"emotion\",\n#     \"context\": \"Situation\",\n#     \"customer\": \"empathetic_dialogues\",\n#     \"agent\": \"labels\",\n# }\n\n# # --------------------------------------------------------\n# # ü™ì Split dataset: 80% train, 10% val, 10% test\n# # --------------------------------------------------------\n# from sklearn.model_selection import train_test_split\n\n# train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n# val_df, test_df   = train_test_split(temp_df, test_size=0.5, random_state=42)\n\n# print(f\"‚úÖ Splits created ‚Äî Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n\n# # --------------------------------------------------------\n# # ‚úÖ Take only 10% of the data for testing\n# # --------------------------------------------------------\n# small_train_df = train_df.sample(frac=0.1, random_state=42)\n# small_val_df   = val_df.sample(frac=0.1, random_state=42)\n\n# print(f\"üì¶ Using subset ‚Äî Train: {len(small_train_df)} | Val: {len(small_val_df)}\")\n\n# # --------------------------------------------------------\n# # üß© Dataset Definition\n# # --------------------------------------------------------\n# class EmpatheticDataset(Dataset):\n#     def __init__(self, df, word2idx, columns):\n#         self.df = df.reset_index(drop=True)\n#         self.word2idx = word2idx\n#         self.columns = columns\n\n#     def encode_text(self, text):\n#         tokens = text.split()\n#         ids = [self.word2idx.get(w, self.word2idx[\"<unk>\"]) for w in tokens]\n#         ids = [self.word2idx[\"<bos>\"]] + ids[:MAX_LEN-2] + [self.word2idx[\"<eos>\"]]\n#         return torch.tensor(ids, dtype=torch.long)\n\n#     def __getitem__(self, idx):\n#         row = self.df.iloc[idx]\n#         src_text = f\"emotion: {row[self.columns['emotion']]} | situation: {row[self.columns['context']]} | customer: {row[self.columns['customer']]}\"\n#         tgt_text = f\"agent: {row[self.columns['agent']]}\"\n#         return self.encode_text(src_text), self.encode_text(tgt_text)\n\n#     def __len__(self):\n#         return len(self.df)\n\n# # --------------------------------------------------------\n# # üß∞ Collate Function (for padding)\n# # --------------------------------------------------------\n# def collate_fn(batch):\n#     src_batch, tgt_batch = zip(*batch)\n#     src_batch = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=pad_idx)\n#     tgt_batch = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=pad_idx)\n#     return src_batch, tgt_batch\n\n# # --------------------------------------------------------\n# # üì¶ DataLoaders\n# # --------------------------------------------------------\n# train_loader = DataLoader(EmpatheticDataset(small_train_df, word2idx, COLUMNS),\n#                           batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n\n# val_loader = DataLoader(EmpatheticDataset(small_val_df, word2idx, COLUMNS),\n#                         batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n# print(\"‚úÖ DataLoaders ready!\")\n\n# # --------------------------------------------------------\n# # üéØ Loss & Optimizer\n# # --------------------------------------------------------\n# criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n# optimizer = optim.Adam(model.parameters(), lr=LR)\n\n# # --------------------------------------------------------\n# # üî• Training Loop\n# # --------------------------------------------------------\n# def run_epoch(model, loader, optimizer=None):\n#     total_loss = 0\n#     train_mode = optimizer is not None\n#     model.train() if train_mode else model.eval()\n\n#     loop = tqdm(loader, desc=\"üß† Training\" if train_mode else \"üß™ Validating\", leave=False)\n#     for src, tgt in loop:\n#         src, tgt = src.to(device), tgt.to(device)\n#         if train_mode:\n#             optimizer.zero_grad()\n\n#         output = model(src, tgt[:, :-1])\n#         loss = criterion(output.reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n#         if train_mode:\n#             loss.backward()\n#             torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n#             optimizer.step()\n\n#         total_loss += loss.item()\n#         loop.set_postfix(loss=loss.item())\n\n#     return total_loss / len(loader)\n\n# # --------------------------------------------------------\n# # üöÄ Run Mini Training\n# # --------------------------------------------------------\n# torch.cuda.empty_cache()\n# gc.collect()\n\n# print(\"\\n===== üöÄ MINI TRAINING START =====\")\n# train_loss = run_epoch(model, train_loader, optimizer)\n# val_loss = run_epoch(model, val_loader)\n# print(f\"\\n‚úÖ Done! Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T10:34:02.693756Z","iopub.execute_input":"2025-10-14T10:34:02.694493Z","iopub.status.idle":"2025-10-14T10:34:14.312357Z","shell.execute_reply.started":"2025-10-14T10:34:02.694463Z","shell.execute_reply":"2025-10-14T10:34:14.311629Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# STEP 7 ‚Äì Full GPU Training + Checkpoint Save","metadata":{}},{"cell_type":"code","source":"# ===============================================================\n# STEP 7: Full GPU Training + Checkpoint Save (Deep Training + ETA)\n# ===============================================================\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom tqdm.auto import tqdm\nimport time\nfrom sklearn.model_selection import train_test_split\nimport os\n\n# ---------------------------------------------------------------\n# ‚öôÔ∏è Device Setup\n# ---------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"üöÄ Using device: {device}\")\n\n# ---------------------------------------------------------------\n# ‚úÖ Check essential variables\n# ---------------------------------------------------------------\nassert \"df\" in globals(), \"‚ùå Dataset 'df' not found in memory!\"\nassert \"word2idx\" in globals(), \"‚ùå Vocabulary not found ‚Äî please rebuild vocabulary first!\"\nassert \"model\" in globals(), \"‚ùå Model not found ‚Äî please run Step 5 first!\"\n\n# ---------------------------------------------------------------\n# üîπ 1. Dataset Split (80/10/10)\n# ---------------------------------------------------------------\ntrain_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\nprint(f\"‚úÖ Splits ‚Äî Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n\n# ---------------------------------------------------------------\n# üîπ 2. Dataset Class\n# ---------------------------------------------------------------\nclass EmpatheticDataset(Dataset):\n    def __init__(self, dataframe, word2idx, max_len=64):\n        self.data = dataframe\n        self.word2idx = word2idx\n        self.max_len = max_len\n\n    def encode_text(self, text):\n        tokens = text.split()\n        ids = [self.word2idx.get(tok, self.word2idx[\"<unk>\"]) for tok in tokens]\n        ids = ids[:self.max_len - 2]\n        return torch.tensor([self.word2idx[\"<bos>\"]] + ids + [self.word2idx[\"<eos>\"]])\n\n    def pad_sequence(self, seq):\n        pad_len = self.max_len - len(seq)\n        if pad_len > 0:\n            seq = torch.cat([seq, torch.full((pad_len,), self.word2idx[\"<pad>\"])])\n        return seq\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        src = f\"emotion: {row['emotion']} | situation: {row['Situation']} | customer: {row['empathetic_dialogues']}\"\n        tgt = f\"{row['labels']}\"\n        src_ids = self.pad_sequence(self.encode_text(src))\n        tgt_ids = self.pad_sequence(self.encode_text(tgt))\n        return src_ids, tgt_ids\n\n    def __len__(self):\n        return len(self.data)\n\n# ---------------------------------------------------------------\n# üîπ 3. DataLoaders\n# ---------------------------------------------------------------\ntrain_dataset = EmpatheticDataset(train_df, word2idx)\nval_dataset = EmpatheticDataset(val_df, word2idx)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\nprint(\"‚úÖ DataLoaders ready!\")\n\n# ---------------------------------------------------------------\n# üîπ 4. Training Setup\n# ---------------------------------------------------------------\ncriterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<pad>\"])\noptimizer = optim.Adam(model.parameters(), lr=3e-4)\nnum_epochs = 10  # üî• deep training\nsave_path = \"/kaggle/working/best_model.pt\"\nbest_val_loss = float(\"inf\")\n\n# ---------------------------------------------------------------\n# üîπ 5. Training Loop with ETA + Progress\n# ---------------------------------------------------------------\nprint(\"\\nüöÄ Starting Deep Training...\\n\")\ntotal_start_time = time.time()\n\nfor epoch in range(1, num_epochs + 1):\n    model.train()\n    train_loss = 0\n    epoch_start = time.time()\n\n    with tqdm(total=len(train_loader), desc=f\"üß† Epoch {epoch}/{num_epochs}\", unit=\"batch\") as pbar:\n        for i, (src, tgt) in enumerate(train_loader):\n            src, tgt = src.to(device), tgt.to(device)\n            optimizer.zero_grad()\n            output = model(src, tgt[:, :-1])\n            loss = criterion(output.reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n            pbar.set_postfix({\"Train Loss\": f\"{loss.item():.4f}\"})\n            pbar.update(1)\n\n    train_loss /= len(train_loader)\n\n    # üîπ Validation Phase\n    model.eval()\n    val_loss = 0\n    with torch.no_grad(), tqdm(total=len(val_loader), desc=f\"üîç Validation {epoch}\", unit=\"batch\") as vbar:\n        for src, tgt in val_loader:\n            src, tgt = src.to(device), tgt.to(device)\n            output = model(src, tgt[:, :-1])\n            loss = criterion(output.reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n            val_loss += loss.item()\n            vbar.set_postfix({\"Val Loss\": f\"{loss.item():.4f}\"})\n            vbar.update(1)\n\n    val_loss /= len(val_loader)\n    epoch_time = time.time() - epoch_start\n    remaining_time = epoch_time * (num_epochs - epoch)\n    eta_min = remaining_time / 60\n\n    print(f\"üìâ Epoch {epoch}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | ‚è±Ô∏è ETA: {eta_min:.1f} min\")\n\n    # üîπ Save best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), save_path)\n        print(f\"üíæ Best model saved ‚Üí {save_path}\")\n\ntotal_time = (time.time() - total_start_time) / 60\nprint(f\"\\n‚úÖ Deep training completed! Total time: {total_time:.2f} min\")\nprint(f\"üèÜ Best Validation Loss: {best_val_loss:.4f}\")\n\n# ---------------------------------------------------------------\n# üîπ 6. Load Best Model for Inference\n# ---------------------------------------------------------------\nmodel.load_state_dict(torch.load(save_path))\nmodel.eval()\nprint(\"‚úÖ Best model loaded and ready for inference!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T11:01:57.312833Z","iopub.execute_input":"2025-10-14T11:01:57.313110Z","iopub.status.idle":"2025-10-14T11:12:43.190056Z","shell.execute_reply.started":"2025-10-14T11:01:57.313087Z","shell.execute_reply":"2025-10-14T11:12:43.189178Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# STEP 8: Inference / Response Generation","metadata":{}},{"cell_type":"code","source":"# ===============================================================\n# STEP 8: Inference + Evaluation + Interactive Chat (Fixed)\n# ===============================================================\n\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom tqdm import tqdm\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nimport math\n\n# ---------------------------------------------------------------\n# ‚öôÔ∏è Setup\n# ---------------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.eval()\nprint(f\"üöÄ Using device: {device}\")\n\n# ---------------------------------------------------------------\n# üîπ Helper Functions\n# ---------------------------------------------------------------\ndef encode_text(text, word2idx, max_len=64):\n    tokens = text.split()\n    ids = [word2idx.get(tok, word2idx[\"<unk>\"]) for tok in tokens]\n    ids = ids[:max_len - 2]\n    ids = [word2idx[\"<bos>\"]] + ids + [word2idx[\"<eos>\"]]\n    pad_len = max_len - len(ids)\n    ids += [word2idx[\"<pad>\"]] * pad_len\n    return torch.tensor(ids).unsqueeze(0).to(device)\n\ndef decode_ids(ids, idx2word):\n    tokens = [idx2word[i] for i in ids if i not in (\n        word2idx[\"<pad>\"], word2idx[\"<bos>\"], word2idx[\"<eos>\"]\n    )]\n    return \" \".join(tokens)\n\n# ---------------------------------------------------------------\n# üîπ Improved Reply Generation (with Temperature + Top-k Sampling)\n# ---------------------------------------------------------------\ndef generate_reply(emotion, situation, customer_text, max_len=50, temperature=0.8, top_k=20):\n    model.eval()\n    idx2word = {v: k for k, v in word2idx.items()}\n\n    with torch.no_grad():\n        src = f\"emotion: {emotion} | situation: {situation} | customer: {customer_text}\"\n        src_tensor = encode_text(src, word2idx)\n        tgt_input = torch.tensor([[word2idx[\"<bos>\"]]], device=device)\n\n        for _ in range(max_len):\n            output = model(src_tensor, tgt_input)        # [1, seq_len, vocab]\n            logits = output[:, -1, :] / temperature      # take last token\n            probs = F.softmax(logits, dim=-1)\n\n            # top-k sampling\n            topk_probs, topk_indices = torch.topk(probs, k=top_k)\n            next_token = topk_indices[0, torch.multinomial(topk_probs[0], 1)].unsqueeze(0)\n\n            tgt_input = torch.cat([tgt_input, next_token.unsqueeze(0)], dim=1)\n            if next_token.item() == word2idx[\"<eos>\"]:\n                break\n\n        reply = decode_ids(tgt_input[0].tolist(), idx2word)\n        return reply\n\n# ---------------------------------------------------------------\n# üîπ Evaluation Metrics (BLEU + Perplexity)\n# ---------------------------------------------------------------\ndef evaluate_model(model, data_loader, word2idx, idx2word):\n    model.eval()\n    total_loss = 0\n    total_bleu = []\n    criterion = torch.nn.CrossEntropyLoss(ignore_index=word2idx[\"<pad>\"])\n\n    with torch.no_grad():\n        for src, tgt in tqdm(data_loader, desc=\"üìä Evaluating\"):\n            src, tgt = src.to(device), tgt.to(device)\n            output = model(src, tgt[:, :-1])\n            loss = criterion(output.reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n            total_loss += loss.item()\n\n            # BLEU score\n            pred_tokens = output.argmax(dim=-1).detach().cpu().numpy()\n            for i in range(pred_tokens.shape[0]):\n                ref = decode_ids(tgt[i].cpu().numpy(), idx2word).split()\n                hyp = decode_ids(pred_tokens[i], idx2word).split()\n                smoothie = SmoothingFunction().method4\n                bleu = sentence_bleu([ref], hyp, smoothing_function=smoothie)\n                total_bleu.append(bleu)\n\n    avg_loss = total_loss / len(data_loader)\n    perplexity = math.exp(avg_loss)\n    avg_bleu = np.mean(total_bleu)\n    return avg_loss, perplexity, avg_bleu\n\n# ---------------------------------------------------------------\n# üîπ Run Evaluation\n# ---------------------------------------------------------------\nprint(\"\\nüìä Running evaluation on validation set...\")\nidx2word = {v: k for k, v in word2idx.items()}\nval_loss, val_ppl, val_bleu = evaluate_model(model, val_loader, word2idx, idx2word)\nprint(f\"‚úÖ Evaluation Complete!\")\nprint(f\"üìâ Val Loss: {val_loss:.4f} | ü§Ø Perplexity: {val_ppl:.2f} | üèÜ BLEU: {val_bleu:.4f}\")\n\n# # ---------------------------------------------------------------\n# # üîπ Interactive Chat Loop\n# # ---------------------------------------------------------------\n# print(\"\\n======================\")\n# print(\"üí¨ Empathetic Chatbot Ready!\")\n# print(\"======================\")\n# print(\"Type 'exit' to stop chatting.\\n\")\n\n# while True:\n#     emo = input(\"Emotion: \").strip()\n#     if emo.lower() == \"exit\":\n#         break\n#     sit = input(\"Situation: \").strip()\n#     if sit.lower() == \"exit\":\n#         break\n#     cust = input(\"You: \").strip()\n#     if cust.lower() == \"exit\":\n#         break\n\n#     reply = generate_reply(emo, sit, cust)\n#     print(f\"ü§ñ Chatbot: {reply}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T11:29:31.123951Z","iopub.execute_input":"2025-10-14T11:29:31.124600Z","iopub.status.idle":"2025-10-14T11:30:11.298949Z","shell.execute_reply.started":"2025-10-14T11:29:31.124575Z","shell.execute_reply":"2025-10-14T11:30:11.297939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================================================\n# STEP 8: Chatbot Inference / Response Generation\n# ===============================================================\n\nimport torch\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.eval()\nprint(f\"üí¨ Using device: {device}\")\n\n# ---------------------------------------------------------------\n# üîπ Helper: Convert text ‚Üí tensor\n# ---------------------------------------------------------------\ndef encode_text(text, word2idx, max_len=64):\n    tokens = text.split()\n    ids = [word2idx.get(tok, word2idx[\"<unk>\"]) for tok in tokens]\n    ids = ids[:max_len - 2]\n    ids = [word2idx[\"<bos>\"]] + ids + [word2idx[\"<eos>\"]]\n    pad_len = max_len - len(ids)\n    ids += [word2idx[\"<pad>\"]] * pad_len\n    return torch.tensor(ids).unsqueeze(0).to(device)\n\n# ---------------------------------------------------------------\n# üîπ Helper: Decode tensor ‚Üí text\n# ---------------------------------------------------------------\ndef decode_ids(ids, idx2word):\n    tokens = [idx2word[i] for i in ids if i not in [\n        word2idx[\"<pad>\"], word2idx[\"<bos>\"], word2idx[\"<eos>\"]]]\n    return \" \".join(tokens)\n\n# ---------------------------------------------------------------\n# üîπ Generate Response\n# ---------------------------------------------------------------\ndef generate_reply(emotion, situation, customer_text, max_len=50):\n    model.eval()\n    with torch.no_grad():\n        src = f\"emotion: {emotion} | situation: {situation} | customer: {customer_text}\"\n        src_tensor = encode_text(src, word2idx)\n        tgt_input = torch.tensor([[word2idx[\"<bos>\"]]]).to(device)\n\n        for _ in range(max_len):\n            output = model(src_tensor, tgt_input)\n            next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(0)\n            tgt_input = torch.cat([tgt_input, next_token], dim=1)\n            if next_token.item() == word2idx[\"<eos>\"]:\n                break\n\n        reply = decode_ids(tgt_input[0].tolist(), {v: k for k, v in word2idx.items()})\n        return reply\n\n# ---------------------------------------------------------------\n# üîπ Test the chatbot\n# ---------------------------------------------------------------\nexample_emotion = \"sad\"\nexample_situation = \"lost my favorite item\"\nexample_customer = \"I can't find it anywhere, I feel terrible.\"\n\nresponse = generate_reply(example_emotion, example_situation, example_customer)\nprint(\"\\nüßç‚Äç‚ôÇÔ∏è User:\", example_customer)\nprint(\"ü§ñ Chatbot:\", response)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-14T11:31:08.761451Z","iopub.execute_input":"2025-10-14T11:31:08.761970Z","iopub.status.idle":"2025-10-14T11:31:08.845366Z","shell.execute_reply.started":"2025-10-14T11:31:08.761947Z","shell.execute_reply":"2025-10-14T11:31:08.844702Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 10 Examples","metadata":{}},{"cell_type":"code","source":"# ===============================================================\n# STEP 8: Chatbot Inference / Response Generation (with 10 examples)\n# ===============================================================\n\nimport torch\nimport torch.nn.functional as F\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.eval()\nprint(f\"üí¨ Using device: {device}\")\n\n# ---------------------------------------------------------------\n# üîπ Helper: Convert text ‚Üí tensor\n# ---------------------------------------------------------------\ndef encode_text(text, word2idx, max_len=64):\n    tokens = text.split()\n    ids = [word2idx.get(tok, word2idx[\"<unk>\"]) for tok in tokens]\n    ids = ids[:max_len - 2]\n    ids = [word2idx[\"<bos>\"]] + ids + [word2idx[\"<eos>\"]]\n    pad_len = max_len - len(ids)\n    ids += [word2idx[\"<pad>\"]] * pad_len\n    return torch.tensor(ids).unsqueeze(0).to(device)\n\n# ---------------------------------------------------------------\n# üîπ Helper: Decode tensor ‚Üí text\n# ---------------------------------------------------------------\ndef decode_ids(ids, idx2word):\n    tokens = [idx2word[i] for i in ids if i not in [\n        word2idx[\"<pad>\"], word2idx[\"<bos>\"], word2idx[\"<eos>\"]]]\n    return \" \".join(tokens)\n\n# ---------------------------------------------------------------\n# üîπ Generate Response\n# ---------------------------------------------------------------\ndef generate_reply(emotion, situation, customer_text, max_len=50):\n    model.eval()\n    with torch.no_grad():\n        src = f\"emotion: {emotion} | situation: {situation} | customer: {customer_text}\"\n        src_tensor = encode_text(src, word2idx)\n        tgt_input = torch.tensor([[word2idx[\"<bos>\"]]]).to(device)\n\n        for _ in range(max_len):\n            output = model(src_tensor, tgt_input)\n            next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(0)\n            tgt_input = torch.cat([tgt_input, next_token], dim=1)\n            if next_token.item() == word2idx[\"<eos>\"]:\n                break\n\n        reply = decode_ids(tgt_input[0].tolist(), {v: k for k, v in word2idx.items()})\n        return reply\n\n\n# ---------------------------------------------------------------\n# üîπ Test the chatbot with 11 diverse examples\n# ---------------------------------------------------------------\ntest_examples = [\n    (\"sad\", \"lost my favorite item\", \"I can't find it anywhere, I feel terrible.\"),\n    (\"angry\", \"argument with a friend\", \"He was so rude to me, I can‚Äôt believe it.\"),\n    (\"happy\", \"got a promotion at work\", \"I just got promoted today!\"),\n    (\"afraid\", \"going for a medical test\", \"I‚Äôm scared of what the doctor will say.\"),\n    (\"disappointed\", \"failed an exam\", \"I studied so hard but still didn‚Äôt pass.\"),\n    (\"surprised\", \"unexpected gift\", \"My friend sent me a present out of nowhere!\"),\n    (\"lonely\", \"moved to a new city\", \"I don‚Äôt know anyone here yet, it feels empty.\"),\n    (\"grateful\", \"someone helped me\", \"She stayed up late just to help me finish my work.\"),\n    (\"embarrassed\", \"made a mistake in public\", \"Everyone saw me trip over the stairs.\"),\n    (\"stressed\", \"too much workload\", \"I have so many deadlines, I can‚Äôt keep up.\"),\n    (\"hopeful\", \"starting a new job\", \"I really want to do well in this new role.\"),\n]\n\nprint(\"\\n==============================\")\nprint(\"üí¨ Testing Chatbot Responses\")\nprint(\"==============================\")\n\nfor i, (emo, sit, cust) in enumerate(test_examples, 1):\n    response = generate_reply(emo, sit, cust)\n    print(f\"\\nüßç‚Äç‚ôÇÔ∏è Example {i}\")\n    print(f\"Emotion: {emo}\")\n    print(f\"Situation: {sit}\")\n    print(f\"You: {cust}\")\n    print(f\"ü§ñ Chatbot: {response}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}